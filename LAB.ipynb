{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Tutorial on Q-Leraning</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gym\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "from Env import Env\n",
    "from GridWord import GridWord\n",
    "from LearningRate import LearningRate\n",
    "from Q import Q\n",
    "from PolicyEpsilonGreedy import PolicyEpsilonGreedy\n",
    "\n",
    "from math import radians\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoritical introduction\n",
    "\n",
    "## Markovian Decision Porcess\n",
    "\n",
    "Before tackling a reinforcement learning problem let's define a modelisation for the agents. The <b>Markovian Decision Process (MDP)</b> is defined as follows:\n",
    "* A <b>state</b> space $\\mathcal{X}$.\n",
    "* An <b>action</b> space $\\mathcal{A}$.\n",
    "* A <b>transition</b> transition probability distribution $p(x'|x,a)$. This corresponds to the dynamics of the problem. \"What is the space when this action is taken while being in this state\".\n",
    "* A <b>reward</b> function $r(x,a)$. \"What do we get from performing this action while being in this state\".\n",
    "\n",
    "This modelisation is Markovian since it follows the <b>Markov property</b>:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbb{P}(X_t = x | X_0, \\dots, X_{t-1}) = \\mathbb{P}(X_t = x | X_{t-1})\n",
    "\\end{equation}\n",
    "\n",
    "In other words, the evolution of the agent only depends on the state he is in, not on the past. This will be usefull to train the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Value function and the Bellman equation\n",
    "\n",
    "Let's define the <b>Value function</b> $V$ as the total reward we can expect when we are in a certain state. This depends on the <b>policy</b> we choose. The policy $\\pi(a|x)$ is defined by the operator, and defines what action is to be taken depending on the state. Then the value function is defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "    V^\\pi(x) = \\mathbb{E}\\Big[\\sum_{t=0}^T \\gamma^t r(x_t, a_t) \\Big| x_0, \\pi \\Big]\n",
    "\\end{equation}\n",
    "Where $\\gamma$ is the discount over time and $T$ is the random final time step.\n",
    "\n",
    "Let's denote by $\\pi^*$ the optimal policy, that is to say the policy that maximizes the value function at each state, and by $V^*$ the resulting value function.\n",
    "\n",
    "If the policy is stationary, meaning that it doesn't change with time, wich is the case for the optimal policy, then the value function follows the <b>Bellman equation</b>:\n",
    "\n",
    "\\begin{equation}\n",
    "    V^*(x) = r(x,\\pi^*(x)) + \\gamma \\sum_{x' \\in \\mathcal{X}} p(x'|x,\\pi^*(x)) V^*(x')\n",
    "\\end{equation}\n",
    "Where $\\pi^*(x)$ is the action returned by the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Instead of trying to learn the value fonction $V(x)$ for all states, Q-learning consists of learning the $Q$ function that associates each couple of state and action with the expected final reward.\n",
    "The associated value function of a $Q$ function under the policy $\\pi$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "    V^{\\pi}(x) = \\sum_{a \\in \\mathcal{A}} \\pi(a|x) Q(x, a)\n",
    "\\end{equation}\n",
    "\n",
    "If we define the optimal policy as the greedy policy $\\pi^*(x) = \\arg\\max_a Q^*(x, a)$. Then the optimal $Q$ function follows the Bellman equation. We can use this to approximate $Q^*$ with the <b>Q-learning</b> algorithm:\n",
    "\n",
    "<b>Initialize</b> $Q_0$ randomly  \n",
    "<b>For</b> $e = 1, \\dots,n$:  \n",
    "> <b>Intiate</b> $t = 0, x_0$  \n",
    "> <b>While</b> $x_t$ <b>not</b> terminal:  \n",
    ">> <b>Choose</b> $a_t$ according to a suitable <b>exploration policy</b>  \n",
    ">> <b>Observe</b> $r_t = r(x_t, a_t)$, $x_{t+1}$  \n",
    ">> <b>Temporal difference</b> $\\delta_{t} = r_t$ <b>If</b> $x_t$ is terminal <b>Else</b> $r_t + \\gamma \\max_{a}Q(x_{t+1}, a) - Q(x_t, a)$  \n",
    ">> <b>Update</b> $Q(x_t, a_t) =  Q(x_t, a_t) + \\alpha(x_t, a_t) \\delta_t$  \n",
    "\n",
    "If the learning rate $\\alpha_t$ satisfies the <b>Robbins-Monro</b> conditions $\\sum_{t=0}^{\\infty} \\alpha_t = \\infty$ and $\\sum_{t=0}^{\\infty} \\alpha_t^2 < \\infty$ for every state action couple, and every state is visited infinitely often then the convergence toward the optimal $Q$ function is guaranted (almost surrely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation of the provided Classes\n",
    "\n",
    "### The first environement we are going to use is a maze:\n",
    "```python\n",
    "GridWord\n",
    "# methods\n",
    "    __init__(path) # Only needs a path to the grid file that describes the maze\n",
    "    reset() # resets the environement to it's original state\n",
    "        return state\n",
    "    step(action) # Performs the requiered action\n",
    "                 # action has too be in [\"up\", \"right\", \"down\", \"left\"]\n",
    "        return state, reward, done # Please note thate the reward is allways -1 in this environement\n",
    "    render_path() # Plots the maze and the path taken by the agent up to now\n",
    "    render_V() # Plots the value function associated with the Q function of the environement\n",
    "    render_path_and_V() # Both.\n",
    "```\n",
    "\n",
    "Note that ```GridWord``` also has an instance of the next ```Q``` class initialized.\n",
    "### The Q function:\n",
    "```python\n",
    "Q\n",
    "# methods\n",
    "    __init__(actions_size, # Can either be a list of the actions or the size of the action space\n",
    "             discrete=True, # If the STATE space is discrete or not\n",
    "             state_shape=[], # If the state space is discrete provide its shape\n",
    "             segmentation=[], # If it is continuous provide a discritization grid\n",
    "             init_range=[0,1]) # Initialization range for Q.\n",
    "    __call__(state, action)\n",
    "        return Q_value\n",
    "    get_V(state) # Returns the value funciton for this state\n",
    "        return V_value\n",
    "```\n",
    "\n",
    "### The $\\epsilon$-Greedy Policy\n",
    "\n",
    "Earlier we talked about a <b>\"suitable exploration ppolicy\"</b>. \n",
    "The $\\epsilon$-greedy policy chooses an action as follows:\n",
    "* With probability $\\epsilon$ returns $\\arg\\max_a Q(x_t, a)$\n",
    "* With probability $1-\\epsilon$ uniformly chooses <u>another</u> state\n",
    "\n",
    "```python\n",
    "PolicyEpsilonGreedy\n",
    "# methods\n",
    "    __init__(Q, action_space, # An instance of the previous class and the action space\n",
    "             epsilon, decay, # The initial epsilon and his decay rate\n",
    "             lower, decay_every) # A lower bound for epsilon and the rate at which decay is performed\n",
    "    __call__(state, be_greedy=False) # If be_greedy is True ignores epsilon and returns the best action\n",
    "        return action                # usefull at test time\n",
    "```\n",
    "\n",
    "### Learning-Rate\n",
    "\n",
    "In Q-learning the learning rate depends on the (state, action) couple. Moreover the  learning rate has to satisfy the Robbins-Monro conditions. Here we are going to use the following learning rate:\n",
    "* $\\alpha(x, a) = \\alpha_0 / (1+t(x,a)\\beta)$\n",
    "\n",
    "Where $t(x, a)$ is the number of time this state action couple has been visited.\n",
    "\n",
    "```python\n",
    "LearningRate\n",
    "# methods\n",
    "    __init__(lr0, decay, min_lr, ...) # The initial learning rate, beta and the minimum lr\n",
    "                                      # The other arguments are the same than those used to define Q\n",
    "    __call__(state, action)\n",
    "        return alpha(x,a)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few theoritical questions\n",
    "\n",
    "* <b> Why do you think it is important to use the $\\epsilon$-greedy policy and not just the greedy policy ?</b>\n",
    "* <b> Why is the learning rate independant for every (state, action) couple ?</b>\n",
    "* <b> Why do you think the Robbins-Monro conditions are important ?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A toy exemple\n",
    "\n",
    "Let's consider a simple exemple to illustrate the principles introduces above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridword = GridWord(\"./GridWordResults/grid.txt\")\n",
    "\n",
    "for action in [\"up\", \"right\", \"right\", \"up\", \"up\", \"left\", \"up\"]:\n",
    "    gridword.step(action)\n",
    "\n",
    "gridword.render_path_and_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please define this problem as a MDP:\n",
    "\n",
    "#### Your answer <b>...</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we are ready to implement the Q-learning alogrithm\n",
    "\n",
    "In the next cell you are asked to implement a function that takes as input a ```Q``` instance, an ```env``` and learns the Q function. You only need the ```PolicyEpsilonGreedy``` and ```LearningRate``` classes for that. Please also return the rewards history so that we can plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(Q_, env, action_space, gamma, print_every_episode, n_episodes, epsilon, epsilon_decay, epsilon_min,\n",
    "              epsilon_decay_every, lr0, lr_decay, min_lr):\n",
    "    policy = PolicyEpsilonGreedy(Q_, action_space, epsilon, epsilon_decay, epsilon_min,\n",
    "                                  epsilon_decay_every)\n",
    "    lr = LearningRate(lr0=lr0, decay=lr_decay, min_lr=min_lr, discrete=Q_.discrete,\n",
    "                      actions_size=action_space, state_shape=gridword.grid.shape)\n",
    "    \n",
    "    '''\n",
    "            YOUR CODE HERE\n",
    "    ''' \n",
    "    return Q, policy, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing the environement\n",
    "gridword = GridWord(\"./GridWordResults/grid.txt\")\n",
    "action_space = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "# You can play with all the parameters\n",
    "# Try to find the best ones (those provided are random)\n",
    "_, policy, rewards = Q_learning(Q_=gridword.Q, env=gridword, action_space=action_space,\n",
    "                                print_every_episode=20, n_episodes=500, gamma=0.6, \n",
    "                                epsilon=0.01, epsilon_decay=0.4, epsilon_min=0.001, epsilon_decay_every=10, \n",
    "                                lr0=4, lr_decay=0.0001, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(utils.rolling_average(rewards, 10))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = gridword.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = policy(state, be_greedy=True)\n",
    "    state, _, done = gridword.step(action)\n",
    "\n",
    "gridword.render_path_and_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more complex problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use <a href=\"https://gym.openai.com\">OpenAI Gym</a>. This is a toolkit for developing and comparing reinforcement learning algorithms. It supports a lot of environement and it is easy to play with.\n",
    "\n",
    "Let's start with a simple environement <b>CartPole</b>. The goal is to balance a stick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cells shows how to set up a gym environement and run it with a random policy\n",
    "# We also save the run in the directory \"./gym-result/test\" so that we can watch the video later.\n",
    "env = Env('CartPole-v1', \"./gym-results/CartPole/test\")\n",
    "_ = env.run(100, policy=\"random\")\n",
    "# Now lets display the run\n",
    "env.display_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, with a random policy the agent quickly looses all it's life fastly reaching a game over. <b>With thus need to implement some clever policy</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please define this problem as a MDP:\n",
    "\n",
    "#### Your answer <b>...</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a more clever policy by going in the same direction than our speed.\n",
    "def policy_smart(state):\n",
    "    if state[3] > 0:\n",
    "        return 1\n",
    "    \n",
    "    elif state[3] <= 0:\n",
    "        return 0\n",
    "\n",
    "env = Env('CartPole-v1', \"./gym-results/CartPole/smart\")\n",
    "rewards = env.run(500, policy=policy_smart)\n",
    "env.display_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand every components lets run some random experiments and visualize the states.\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "states = []\n",
    "actions = []\n",
    "for n_samples in range(1000):\n",
    "    env.reset()\n",
    "    for _ in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "env.close()\n",
    "\n",
    "component_names = [\"x_pos\", \"x_vel\", \"theta\", \"theta_vel\"]\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "for _ in range(4):\n",
    "    plt.subplot(2,4,_+1)\n",
    "    plt.plot([state[_] for state in states]) \n",
    "    plt.title(\"State component %i, %s\" % (_, component_names[_]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initializing the environement\n",
    "env = gym.make('CartPole-v0')\n",
    "action_space = [0,1]\n",
    "\n",
    "# Try to find a suitable segmentation\n",
    "# If the list is empty it means you want to map all possible value of the states dimension to the same Q_values\n",
    "segmentation = [[],\n",
    "                [],\n",
    "                [-1,1]\n",
    "                [-1,1]]\n",
    "\n",
    "Q_ = Q(init_range=[0,0],\n",
    "       discrete=False, segmentation=segmentation, actions_size=len(action_space)) \n",
    "\n",
    "_, policy, rewards = Q_learning(Q_=Q_, env=env, action_space=len(action_space),\n",
    "                                print_every_episode=20, n_episodes=500, gamma=0.6, \n",
    "                                epsilon=0.01, epsilon_decay=0.4, epsilon_min=0.001, epsilon_decay_every=10, \n",
    "                                lr0=4, lr_decay=0.0001, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(utils.rolling_average(rewards, 50))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env('CartPole-v0', \"./gym-results/CartPole/RL\")\n",
    "_ = env.run(200, policy)\n",
    "env.display_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets land a space craft !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env('LunarLander-v2', \"./gym-results/LunarLander/random\")\n",
    "_ = env.run(100, policy=\"random\")\n",
    "env.display_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "states = []\n",
    "actions = []\n",
    "for n_samples in range(100):\n",
    "    env.reset()\n",
    "    for _ in range(100):\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "env.close()\n",
    "\n",
    "component_names = [\"x_pos\", \"y_pos\", \"x_vel\", \"y_vel\", \"angle\", \"angular_vel\", \"leg_1_sttus\", \"leg_2_status\"]\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "for _ in range(8):\n",
    "    plt.subplot(2,4,_+1)\n",
    "    plt.plot([state[_] for state in states]) \n",
    "    plt.title(\"State component %i, %s\" % (_, component_names[_]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "action_space = [0,1,2,3]\n",
    "\n",
    "segmentation = [[-1, 0, 1],\n",
    "                [-0.5, 0, 1, 1.5],\n",
    "                [-1, 0, 1],\n",
    "                [-1,0],\n",
    "                [-2,0,2],\n",
    "                [-4, -2, 0, 2, 4],\n",
    "                [],\n",
    "                []]\n",
    "\n",
    "Q_ = Q(init_range=[0,0],\n",
    "       discrete=False, segmentation=segmentation, actions_size=len(action_space)) \n",
    "\n",
    "_, policy, rewards = Q_learning(Q_=Q_, env=env, action_space=len(action_space),\n",
    "                                print_every_episode=50, n_episodes=1000, gamma=0.99,\n",
    "                                epsilon=0.5, epsilon_decay=0.9, epsilon_min=0.1, epsilon_decay_every=50, \n",
    "                                lr0=1, lr_decay=0.1, min_lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(utils.rolling_average(rewards, 50))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env('LunarLander-v2', \"./gym-results/LunarLander/smart\")\n",
    "_ = env.run(200, policy)\n",
    "env.display_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_toy",
   "language": "python",
   "name": "reinforcement_toy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
